{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet architecture\n",
    "\n",
    "We will try to first build our model based on the Unet architecture because from our personal research, this architecture seems to be commonly used for image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>objects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517</td>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\princ\\.cache...</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>{'id': [20602, 20603, 20604, 20605, 20606], 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1030</td>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\princ\\.cache...</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>{'id': [14034, 14035, 14036, 14037, 14038, 140...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217</td>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\princ\\.cache...</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>{'id': [16678, 16679, 16680, 16681, 16682, 166...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                              image  width  height  \\\n",
       "0      1517  {'bytes': None, 'path': 'C:\\Users\\princ\\.cache...    500     500   \n",
       "1      1030  {'bytes': None, 'path': 'C:\\Users\\princ\\.cache...    500     500   \n",
       "2      1217  {'bytes': None, 'path': 'C:\\Users\\princ\\.cache...    500     500   \n",
       "\n",
       "                                             objects  \n",
       "0  {'id': [20602, 20603, 20604, 20605, 20606], 'a...  \n",
       "1  {'id': [14034, 14035, 14036, 14037, 14038, 140...  \n",
       "2  {'id': [16678, 16679, 16680, 16681, 16682, 166...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "image_train_dataset = load_dataset(\n",
    "'keremberke/satellite-building-segmentation',\n",
    "split='train', name='mini'\n",
    ").to_pandas()\n",
    "image_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes needed for processing data (personal dataSet class for Torch)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im = self.dataframe.iloc[index]\n",
    "        image_path = im['image']['path']\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            real_img = self.transform(img)\n",
    "\n",
    "        segmentation = self.parse_segmentation(im['objects'], img.size)\n",
    "\n",
    "        #print(im['image'])\n",
    "        # with index = 1 return the path of the first image of dataset\n",
    "\n",
    "        #print(im['objects'])\n",
    "        # Full dataframe, not sure what to use it for right now (contains id_objects [building or not, etc...])\n",
    "        # See next markdown cell to see typical representation\n",
    "        return {'image' : real_img, 'segmentation': segmentation }\n",
    "    \n",
    "\n",
    "    def parse_segmentation(self, object, size):\n",
    "        \n",
    "        segmentation_mask = np.zeros(size, dtype=np.uint8)\n",
    "\n",
    "        for obj in object['segmentation']:\n",
    "            for polygon in obj:\n",
    "                polygon = np.array(polygon, dtype=np.int32)\n",
    "                polygon = polygon.reshape((-1, 2))\n",
    "                cv2.fillPoly(segmentation_mask, [polygon], 1)\n",
    "\n",
    "        return segmentation_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data contained in 'objects' field for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'id': array([14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042],\n",
    "      dtype=int64), 'area': array([31164, 29682,  6802,  6863, 17415, 45449,  7452,  7105,  4062],\n",
    "      dtype=int64), 'bbox': array([array([236.  , 350.  , 207.36, 150.29], dtype=float32),\n",
    "       array([  0.  , 133.  , 191.28, 155.18], dtype=float32),\n",
    "       array([429.  ,  78.  ,  70.98,  95.83], dtype=float32),\n",
    "       array([432.  , 268.  ,  67.75, 101.3 ], dtype=float32),\n",
    "       array([295.  , 135.  , 120.48, 144.55], dtype=float32),\n",
    "       array([ 46.  , 221.  , 227.5 , 199.78], dtype=float32),\n",
    "       array([341.  ,   0.  ,  87.23,  85.44], dtype=float32),\n",
    "       array([198.  ,  86.  ,  87.67,  81.05], dtype=float32),\n",
    "       array([447.  , 362.  ,  52.53,  77.33], dtype=float32)],\n",
    "      dtype=object), 'segmentation': array([array([array([377.6 , 500.  , 376.23, 490.68, 371.24, 485.31, 443.08, 432.72,\n",
    "                     349.77, 349.71, 265.06, 391.88, 328.69, 446.58, 235.72, 500.  ,\n",
    "                     377.6 , 500.  ], dtype=float32)                                ],\n",
    "             dtype=object)                                                            ,\n",
    "       array([array([9.5200e+00, 2.8832e+02, 2.0440e+01, 2.7899e+02, 1.7660e+01,\n",
    "                     2.7638e+02, 2.1910e+01, 2.7333e+02, 2.4110e+01, 2.7469e+02,\n",
    "                     4.7520e+01, 2.5993e+02, 5.0120e+01, 2.6417e+02, 5.8830e+01,\n",
    "                     2.5761e+02, 6.0760e+01, 2.6061e+02, 8.6430e+01, 2.4149e+02,\n",
    "                     8.9540e+01, 2.4545e+02, 1.0594e+02, 2.3595e+02, 1.0181e+02,\n",
    "                     2.3103e+02, 1.2749e+02, 2.1848e+02, 1.3099e+02, 2.2504e+02,\n",
    "                     1.4490e+02, 2.1520e+02, 1.4903e+02, 2.2068e+02, 1.8720e+02,\n",
    "                     1.9885e+02, 1.9128e+02, 1.8686e+02, 1.4886e+02, 1.3314e+02,\n",
    "                     1.3000e-01, 2.1344e+02, 0.0000e+00, 2.7890e+02, 9.5200e+00,\n",
    "                     2.8832e+02], dtype=float32)                                ],\n",
    "             dtype=object)                                                        ,\n",
    "       array([array([500.  ,  78.07, 434.99, 109.69, 429.02, 123.13, 461.76, 170.73,\n",
    "                     468.8 , 166.8 , 474.4 , 173.9 , 481.1 , 167.8 , 492.87, 165.13,\n",
    "                     500.  , 170.85, 500.  ,  78.07], dtype=float32)                ],\n",
    "             dtype=object)                                                            ,\n",
    "       array([array([500.  , 267.6 , 487.89, 275.17, 490.48, 278.11, 478.73, 286.92,\n",
    "                     478.56, 294.01, 473.89, 292.8 , 465.77, 297.81, 464.21, 295.39,\n",
    "                     453.67, 301.26, 446.6 , 304.2 , 446.3 , 309.5 , 442.7 , 313.3 ,\n",
    "                     439.85, 310.94, 432.25, 315.61, 445.5 , 331.4 , 470.6 , 357.2 ,\n",
    "                     473.72, 355.  , 485.8 , 368.9 , 500.  , 361.41, 500.  , 267.6 ],\n",
    "                    dtype=float32)                                                   ],\n",
    "             dtype=object)                                                             ,\n",
    "       array([array([331.33, 147.38, 311.6 , 158.  , 321.82, 172.66, 316.06, 176.28,\n",
    "                     341.73, 215.4 , 296.93, 243.18, 295.47, 249.16, 321.74, 279.38,\n",
    "                     381.33, 244.86, 364.52, 222.67, 411.22, 197.17, 415.95, 185.73,\n",
    "                     379.87, 134.83, 357.47, 142.74, 345.3 , 147.77, 331.33, 147.38],\n",
    "                    dtype=float32)                                                   ],\n",
    "             dtype=object)                                                             ,\n",
    "       array([array([ 46.2 , 331.58,  86.33, 380.87, 102.47, 369.44, 143.32, 420.93,\n",
    "                     193.  , 390.2 , 151.76, 342.42, 197.83, 314.3 , 188.6 , 302.5 ,\n",
    "                     195.5 , 297.8 , 203.6 , 307.43, 208.  , 304.8 , 216.6 , 314.66,\n",
    "                     212.67, 317.76, 223.  , 330.25, 234.66, 322.75, 235.88, 324.87,\n",
    "                     273.7 , 302.76, 270.44, 285.87, 226.3 , 221.15, 161.26, 260.39,\n",
    "                      59.79, 316.27,  46.2 , 331.58], dtype=float32)                ],\n",
    "             dtype=object)                                                            ,\n",
    "       array([array([373.74,  81.64, 378.22,  78.53, 382.2 ,  85.44, 399.47,  72.29,\n",
    "                     402.23,  76.63, 415.38,  68.85, 411.76,  62.11, 426.95,  55.02,\n",
    "                     428.3 ,  47.9 , 391.38,   0.  , 341.07,  25.5 , 370.99,  61.08,\n",
    "                     364.39,  64.69, 373.74,  81.64], dtype=float32)                ],\n",
    "             dtype=object)                                                            ,\n",
    "       array([array([221.95, 159.74, 224.89, 158.36, 232.84, 166.83, 241.65, 161.3 ,\n",
    "                     244.41, 165.1 , 250.98, 160.09, 254.61, 165.79, 280.18, 148.85,\n",
    "                     275.86, 143.84, 285.37, 136.59, 253.57,  96.67, 239.06,  85.78,\n",
    "                     231.97,  91.31, 228.86,  86.13, 209.51,  95.11, 209.3 , 108.4 ,\n",
    "                     203.6 , 113.  , 206.5 , 115.9 , 197.7 , 121.3 , 207.43, 132.09,\n",
    "                     206.92, 144.53, 221.95, 159.74], dtype=float32)                ],\n",
    "             dtype=object)                                                            ,\n",
    "       array([array([500.  , 362.2 , 448.67, 389.52, 447.47, 401.76, 458.22, 410.72,\n",
    "                     455.54, 422.96, 472.26, 433.71, 500.  , 439.53, 500.  , 384.03,\n",
    "                     500.  , 362.2 ], dtype=float32)                                ],\n",
    "             dtype=object)                                                            ],\n",
    "      dtype=object), 'category': array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes needed for the model definition\n",
    "\n",
    "class layer_conv2(nn.Module):\n",
    "    # Give as parameter the input and output channel\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(layer_conv2, self).__init__()\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channel, out_channels=output_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class begin_model(nn.Module):\n",
    "    # Give as parameter the input and output channel\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(begin_model, self).__init__()\n",
    "        self.first_step = layer_conv2(input_channel, output_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_step(x)\n",
    "        return x\n",
    "    \n",
    "class end_model(nn.Module):\n",
    "    # Give as parameter the input and output channel\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(end_model, self).__init__()\n",
    "        self.final_step = nn.Conv2d(in_channels=input_channel, out_channels=output_channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.final_step(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Classes necessary for 'creating' the U shape of the architecture\n",
    "    \n",
    "class down(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(down, self).__init__()\n",
    "        self.md = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            layer_conv2(input_channel, output_channel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"DOWN - > x size: {x.size()}\")\n",
    "        x = self.md(x)\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(input_channel // 2, input_channel // 2, 2, stride=2)\n",
    "        self.conv = layer_conv2(input_channel, output_channel)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        print(f\"UP -> x1 size: {x1.size()}\")\n",
    "        print(f\"UP -> x2 size: {x2.size()}\")\n",
    "        # Upsample x1 to match the size of x2\n",
    "        x1 = self.up(x1)\n",
    "        # Ensure that the spatial dimensions match after upsampling\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = torch.nn.functional.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class complete_model(nn.Module):\n",
    "    # We give the number of classes expected and the number of channels\n",
    "    def __init__(self, num_chan, num_class):\n",
    "        super(complete_model, self).__init__()\n",
    "        self.debut = begin_model(num_chan, output_channel=64)\n",
    "        self.d1 = down(input_channel=64, output_channel=128)\n",
    "        self.d2 = down(input_channel=128, output_channel=256)\n",
    "        self.d3 = down(input_channel=256,output_channel=512)\n",
    "        self.d4 = down(input_channel=512,output_channel=512)\n",
    "        self.u1 = up(input_channel=1024, output_channel=256,bilinear=False)\n",
    "        self.u2 = up(input_channel=512, output_channel=128,bilinear=False)\n",
    "        self.u3 = up(input_channel=256, output_channel=64,bilinear=False)\n",
    "        self.u4 = up(input_channel=128, output_channel=64,bilinear=False)\n",
    "        self.fin = end_model(input_channel=64, output_channel=num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.debut(x)\n",
    "        x1 = self.d1(x)\n",
    "        x2 = self.d2(x1)\n",
    "        x = self.u1(x1, x2)\n",
    "        x = self.u2(x, x1)\n",
    "        x = self.fin(x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.7686, 0.7608, 0.7686,  ..., 0.5922, 0.6157, 0.6863],\n",
       "          [0.7686, 0.7725, 0.7843,  ..., 0.5804, 0.5608, 0.5882],\n",
       "          [0.7647, 0.7765, 0.7882,  ..., 0.6314, 0.6000, 0.6039],\n",
       "          ...,\n",
       "          [0.6980, 0.6863, 0.6863,  ..., 0.4510, 0.4471, 0.4392],\n",
       "          [0.6941, 0.6863, 0.6863,  ..., 0.4549, 0.4510, 0.4471],\n",
       "          [0.6784, 0.6706, 0.6784,  ..., 0.4627, 0.4588, 0.4510]],\n",
       " \n",
       "         [[0.6471, 0.6392, 0.6471,  ..., 0.5451, 0.5686, 0.6392],\n",
       "          [0.6471, 0.6510, 0.6627,  ..., 0.5333, 0.5137, 0.5412],\n",
       "          [0.6431, 0.6549, 0.6667,  ..., 0.5843, 0.5529, 0.5569],\n",
       "          ...,\n",
       "          [0.6314, 0.6196, 0.6196,  ..., 0.3843, 0.3804, 0.3725],\n",
       "          [0.6275, 0.6196, 0.6196,  ..., 0.3882, 0.3843, 0.3804],\n",
       "          [0.6118, 0.6039, 0.6118,  ..., 0.3961, 0.3922, 0.3843]],\n",
       " \n",
       "         [[0.5255, 0.5176, 0.5255,  ..., 0.4902, 0.5137, 0.5843],\n",
       "          [0.5255, 0.5294, 0.5412,  ..., 0.4784, 0.4588, 0.4863],\n",
       "          [0.5216, 0.5333, 0.5451,  ..., 0.5294, 0.4980, 0.5020],\n",
       "          ...,\n",
       "          [0.5294, 0.5176, 0.5176,  ..., 0.3216, 0.3176, 0.3098],\n",
       "          [0.5255, 0.5176, 0.5176,  ..., 0.3255, 0.3216, 0.3176],\n",
       "          [0.5098, 0.5020, 0.5098,  ..., 0.3333, 0.3294, 0.3216]]]),\n",
       " 'segmentation': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the dataframe\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((500, 500)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_df = MyDataset(image_train_dataset, transform)\n",
    "\n",
    "train_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking only for buildings first so only two classes (building, not building)\n",
    "# channels = 3 because RGB colors \n",
    "model = complete_model(num_chan=3, num_class=2)\n",
    "# Using Adam as optimizer because the TPs did so\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Loss function with Binary Cross-Entropy because two classes at first (building, not building)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complete_model(\n",
       "  (debut): begin_model(\n",
       "    (first_step): layer_conv2(\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (d1): down(\n",
       "    (md): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): layer_conv2(\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (d2): down(\n",
       "    (md): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): layer_conv2(\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (d3): down(\n",
       "    (md): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): layer_conv2(\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (d4): down(\n",
       "    (md): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): layer_conv2(\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (u1): up(\n",
       "    (up): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): layer_conv2(\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (u2): up(\n",
       "    (up): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): layer_conv2(\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (u3): up(\n",
       "    (up): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): layer_conv2(\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (u4): up(\n",
       "    (up): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): layer_conv2(\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fin): end_model(\n",
       "    (final_step): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN - > x size: torch.Size([2, 64, 500, 500])\n",
      "DOWN - > x size: torch.Size([2, 128, 250, 250])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/2 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP -> x1 size: torch.Size([2, 128, 250, 250])\n",
      "UP -> x2 size: torch.Size([2, 256, 125, 125])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [512, 512, 2, 2], expected input[2, 128, 250, 250] to have 512 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 103\u001b[0m, in \u001b[0;36mcomplete_model.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    101\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md1(x)\n\u001b[0;32m    102\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2(x1)\n\u001b[1;32m--> 103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu2(x, x1)\n\u001b[0;32m    105\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfin(x)\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 73\u001b[0m, in \u001b[0;36mup.forward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUP -> x2 size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx2\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Upsample x1 to match the size of x2\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Ensure that the spatial dimensions match after upsampling\u001b[39;00m\n\u001b[0;32m     75\u001b[0m diffY \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x1\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\princ\\pro\\FAC\\Deep_learrning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [512, 512, 2, 2], expected input[2, 128, 250, 250] to have 512 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loader = torch.utils.data.DataLoader(train_df, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):   \n",
    "    model.train()\n",
    "    acc_for_avergage_loss = 0.0\n",
    "    for batch in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        input = batch[\"image\"]\n",
    "        target = batch[\"segmentation\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        acc_for_avergage_loss += loss.item()\n",
    "\n",
    "    # Print the loss at the end of each epoch\n",
    "    average_loss = acc_for_avergage_loss / len(train_loader)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, average_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
